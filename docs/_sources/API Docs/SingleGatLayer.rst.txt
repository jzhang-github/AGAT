##############
SingleGatLayer
##############

Single graph attention network for predicting crystal properties.

.. Note:: Some abbreviations used in ``GATLayer`` class:

   ===============  =================
   Abbreviations    Full name
   ===============  =================
   dist             distance matrix
   feat             features
   ft               features
   src              source node
   dst              destination node
   e                e_i_j: refer to: https://arxiv.org/abs/1710.10903
   a                alpha_i_j: refer to: https://arxiv.org/abs/1710.10903
   att              attention mechanism
   act              activation function
   ===============  =================


.. class:: GATLayer()

   .. method:: __init__(self, num_out, num_heads, bias=False, negative_slope=0.2, activation=None, batch_normalization=False)
   
      :param int num_out: Depth of node representation in the output of each head of this `GAT` layer.
      :param int num_heads: Number of attention heads.
      :param bool bias: Whether the dense layer uses a bias vector.
      :param float negative_slope: Negative slope coefficient of the leakeyrelu activation function.
      :param str/tf.keras.activations activation: activation function to use
      :param bool batch_normalization: if True, append a ``tf.keras.layers.BatchNormalization()`` layer to this AGAT layer.
      
      
   .. method:: build(self, input_shape)
   
      The `build` method is thoroughly introduced by `TensorFlow`. https://www.tensorflow.org/guide/keras/custom_layers_and_models
      In short, this method creates variables at the first ``__call__`` of this class. 
      
      
   .. method:: call(self, feat, dist, graph)
   
      Forward this `GAT` layer.
      
      :param tf.Tensor feat: input features of all nodes (atoms).
      :param tf.Tensor dist: distance matrix.
      :param DGL.graph graph: A graph built with DGL.
      
      :Returns: dst: output features of all nodes.



